В рамках проекта был настроен полный цикл CI/CD для веб-приложения на Django с использованием Kubernetes, Helm, Terraform, Ansible и системы мониторинга на базе Prometheus + Grafana + Loki.
Проект демонстрирует:
- развёртывание инфраструктуры как кода (IaC);
- автоматизацию настройки серверов;
- сборку и деплой приложения в Kubernetes через CI/CD;
- мониторинг и логирование приложения и инфраструктуры.
После создания серверов с помощью Ansible на них устанавливаются все необходимые пакеты и утилиты (Docker, kubectl, helm и др.).
## Архитектура проекта
В качестве облачной платформы выбран Yandex Cloud. С помощью Terraform автоматически развёрнуты три виртуальные машины:
 • master: главная нода (master) Kubernetes-кластера,
 • worker: рабочая нода Kubernetes-кластера,
 • srv:  Служебный сервер (srv)
Отдельный сервер, на котором размещены:
- CI/CD runner (GitHub Actions self-hosted)
- Prometheus
- Grafana
- Loki
- Promtail
> Важно: мониторинг вынесен за пределы Kubernetes-кластера, чтобы сохранялась наблюдаемость даже при падении кластера.

## Используемые технологии

- Terraform — описание облачной инфраструктуры
- Ansible — автоматизация установки ПО на серверах
- Docker / Docker Compose
- Kubernetes (kubeadm)
- Helm — деплой приложения
- GitHub Actions — CI/CD
- Prometheus — сбор метрик
- Grafana — визуализация
- Loki + Promtail — сбор и просмотр логов
- PostgreSQL — база данных приложения
- Django — веб-приложение

## Структура репозитория
.
├── terraform/            # Terraform конфигурации инфраструктуры
├── ansible/              # Ansible роли и плейбуки
├── helm/
│   └── testapp/           # Helm-чарт приложения
│       ├── Chart.yaml
│       ├── values.yaml
│       └── templates/
├── .github/workflows/     # CI/CD пайплайны GitHub Actions
│   ├── build.yml          # Сборка и публикация Docker-образа
│   └── deploy.yml         # Деплой приложения в Kubernetes
├── Dockerfile             # Dockerfile приложения
├── docker-compose.yml     # Используется как референс
├── app/                   # Исходный код Django-приложения
└── README.md
## Kubernetes-кластер:
Kubernetes-кластер установлен вручную с использованием kubeadm (не использовался managed Kubernetes-сервис). Для сетевого взаимодействия подов применён плагин Flannel (CNI). Обе ноды кластера – master и worker – успешно присоединены к кластеру и находятся в статусе Ready. Приложение развёрнуто в отдельном namespace testapp на рабочей ноде.
## Приложение:
Развёрнутое приложение представляет собой простое веб-приложение на фреймворке Django, использующее базу данных PostgreSQL. Репозиторий проекта содержит Dockerfile для сборки контейнера приложения и Helm-чарт для его деплоя в кластер. Сборка Docker-образа запускается автоматически при каждом push в репозиторий или при создании нового тега на GitHub. Собранный образ публикуется в GitLab Container Registry.
CI/CD:
Для реализации конвейера CI/CD используются GitHub Actions. Настроено два workflow:
 • build.yml – собирает Docker-образ приложения и загружает его в GitLab Container Registry;
 • deploy.yml – деплоит приложение в Kubernetes-кластер с помощью Helm (запускается после успешной сборки образа).
Оба workflow выполняются на собственном runner-е GitHub Actions, развернутом на сервере srv (self-hosted runner). Это позволяет процессам деплоя выполнять команды непосредственно в инфраструктуре (например, деплой в кластер через kubectl/helm с сервера srv).
## Мониторинг и логирование:
На сервере srv развёрнут стек мониторинга и логирования с использованием Prometheus, Grafana, Loki и Promtail (все сервисы запущены через Docker Compose).
 • Grafana доступна по адресу http://62.84.115.239:3000 (для визуализации метрик и логов).
 • Prometheus доступен по адресу http://62.84.115.239:9090 (сбор метрик).
 • Для сбора системных метрик серверов и кластера установлен Node Exporter. В Grafana импортирован готовый дашборд Node Exporter Full для наглядного отображения параметров нод.
 • Promtail собирает логи с серверов (в том числе из файлов /var/log/syslog, /var/log/auth.log и др.) и отправляет их в систему логирования Loki. Это позволяет просматривать как системные логи, так и логи приложения через интерфейс Grafana (Loki добавлен в Grafana как источник данных).
 Grafana:
http://62.84.115.239:3000

Prometheus:
http://62.84.115.239:9090

Loki:
http://62.84.115.239:3100
##  Доступ к приложению:
Приложение доступно извне по адресу http://158.160.104.91:30033. При обращении к этому URL происходит автоматическое перенаправление на страницу авторизации Django (/admin/login), поскольку в настройках приложения предусмотрено перенаправление неавторизованных пользователей на административную панель (для демонстрации работы).


## Этап 1. Инфраструктура (Terraform)

Инфраструктура описана с помощью Terraform:
 • создаются виртуальные машины в облаке;
 • формируется Kubernetes-кластер (1 master + 1 worker);
 • создаётся отдельный сервер srv.

Terraform state не хранится в репозитории и исключён через .gitignore.

## Этап 2. Автоматизация (Ansible)

Ansible используется для:
 • установки Docker и Docker Compose;
 • установки kubectl и Helm;
 • настройки доступа по SSH;
 • подготовки сервера srv для мониторинга и CI/CD.

После выполнения Ansible-плейбуков серверы готовы к работе с минимальным количеством ручных действий.

## Этап 3. CI/CD

Сборка образа

Используется GitHub Actions:
 • при пуше в main собирается Docker-образ с тегом latest;
 • при создании git-тега vX.Y.Z образ собирается с соответствующим тегом;
 • образ публикуется в Docker Registry.

Деплой

После успешной сборки:
 • запускается deploy-workflow;
 • Helm-чарт применяется к Kubernetes-кластеру;
 • используется свежесобранный Docker-образ.

Приложение становится доступно по IP-адресу кластера и NodePort.
 ## Этап 4. Helm-чарт

Приложение описано в Helm-чарте:
 • Deployment для Django-приложения;
 • Service для доступа;
 • StatefulSet для PostgreSQL;
 • PVC для хранения данных БД;
 • Secrets для чувствительных данных.

Helm-чарт хранится в репозитории и используется как локально, так и в CI/CD.
## Этап 5. Мониторинг и логирование

Метрики
 • Prometheus собирает метрики с серверов и инфраструктуры;
 • подключены node_exporter для мониторинга ресурсов;
 • Grafana используется для отображения дашбордов.

Логи
 • Promtail собирает системные логи сервера (/var/log/*);
 • Loki хранит и индексирует логи;
 • логи доступны в Grafana Explore.

Результат
 • Приложение автоматически собирается и деплоится в Kubernetes.
 • Мониторинг и логирование работают вне кластера.
 • Инфраструктура полностью описана кодом.
 • CI/CD пайплайн воспроизводим и документирован.
