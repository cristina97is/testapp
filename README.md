Проект: Полный цикл CI/CD в Kubernetes
 1. Описание инфраструктуры:
В качестве облачной платформы выбран Yandex Cloud. С помощью Terraform автоматически развёрнуты три виртуальные машины:
 • master: главная нода (master) Kubernetes-кластера,
 • worker: рабочая нода Kubernetes-кластера,
 • srv: вспомогательный сервер для задач CI/CD и мониторинга.
После создания серверов с помощью Ansible на них устанавливаются все необходимые пакеты и утилиты (Docker, kubectl, helm и др.).
 2. Kubernetes-кластер:
Kubernetes-кластер установлен вручную с использованием kubeadm (не использовался managed Kubernetes-сервис). Для сетевого взаимодействия подов применён плагин Flannel (CNI). Обе ноды кластера – master и worker – успешно присоединены к кластеру и находятся в статусе Ready. Приложение развёрнуто в отдельном namespace testapp на рабочей ноде.
 3. Приложение:
Развёрнутое приложение представляет собой простое веб-приложение на фреймворке Django, использующее базу данных PostgreSQL. Репозиторий проекта содержит Dockerfile для сборки контейнера приложения и Helm-чарт для его деплоя в кластер. Сборка Docker-образа запускается автоматически при каждом push в репозиторий или при создании нового тега на GitHub. Собранный образ публикуется в GitLab Container Registry.
 4. CI/CD:
Для реализации конвейера CI/CD используются GitHub Actions. Настроено два workflow:
 • build.yml – собирает Docker-образ приложения и загружает его в GitLab Container Registry;
 • deploy.yml – деплоит приложение в Kubernetes-кластер с помощью Helm (запускается после успешной сборки образа).
Оба workflow выполняются на собственном runner-е GitHub Actions, развернутом на сервере srv (self-hosted runner). Это позволяет процессам деплоя выполнять команды непосредственно в инфраструктуре (например, деплой в кластер через kubectl/helm с сервера srv).
 5. Мониторинг и логирование:
На сервере srv развёрнут стек мониторинга и логирования с использованием Prometheus, Grafana, Loki и Promtail (все сервисы запущены через Docker Compose).
 • Grafana доступна по адресу http://62.84.115.239:3000 (для визуализации метрик и логов).
 • Prometheus доступен по адресу http://62.84.115.239:9090 (сбор метрик).
 • Для сбора системных метрик серверов и кластера установлен Node Exporter. В Grafana импортирован готовый дашборд Node Exporter Full для наглядного отображения параметров нод.
 • Promtail собирает логи с серверов (в том числе из файлов /var/log/syslog, /var/log/auth.log и др.) и отправляет их в систему логирования Loki. Это позволяет просматривать как системные логи, так и логи приложения через интерфейс Grafana (Loki добавлен в Grafana как источник данных).
 6. Доступ к приложению:
Приложение доступно извне по адресу http://158.160.104.91:30033. При обращении к этому URL происходит автоматическое перенаправление на страницу авторизации Django (/admin/login), поскольку в настройках приложения предусмотрено перенаправление неавторизованных пользователей на административную панель (для демонстрации работы).
